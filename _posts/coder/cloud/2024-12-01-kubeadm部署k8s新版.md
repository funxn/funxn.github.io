---
layout: post
title: kubeadm部署k8s新版
categories: [coder, cloud]
tags: [k8s]
date: 2024-12-01 17:00 +0800
---

## 〇、集群规划

当前使用的是k8s最新稳定版: v1.24.3

本地环境我们预计使用3台2核2G虚拟机，搭建一个1master-2node集群，各虚机节点地址如下：

```shell
192.168.56.109 master
192.168.56.105 node1
192.168.56.106 node2
```

为了方便操作，我们先使用一台debian11.3虚机（已部署好docker、使用samba共享目录）来作为master安装配置好环境，另外2台node节点则可以直接使用virtualbox的“复制-链接复制”模式快速创建，最小化3台虚机所需的重复配置操作！

## 一、初始系统配置

由于 k8s 集群中的每个 node 的 hostname 不能一样，所以需要确保虚拟机的 hostname 不同，首先设置新主机名：

```shell
hostnamectl set-hostname master
```

然后修改`/etc/hosts`，确保域名解析正确：

```shell
127.0.0.1       localhost
192.168.56.109 master
192.168.56.105 node1
192.168.56.106 node2
...
```

> 注意：如果没有修改/etc/hosts，使用`sudo`指令会由于解析不到master而超时报错

接着是一系列系统配置：

```bash
#加载默认的内核模块
cat << EOF > /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter

#正确设置系统参数
cat << EOF > /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
user.max_user_namespaces=28633
vm.swappiness=0
EOF
sysctl -p

#关闭swap，kubenet新版要求禁用swap以提高效率
swapoff -a
sed -ri "/swap/s|^(.*)|#\1|g" /etc/fstab
#确认swap已经关闭
free -m
```

然后，我们需要配置containerd，它是容器运行时，在v1.24.0之后作为k8s默认的容器运行时（不再支持docker-cli）。kubelet启动容器的发展历程摘抄如下：

> 早期: kubelet --> docker-manager --> docker
> 中期: kubelet -CRI-> docker-shim --> docker --> containerd --> runc
> 中期: kubelet -CRI-> cri-containerd --> containerd --> runc
> 当前: kubelet -CRI-> containerd(CRI plugin) --> runc
> 当前: kubelet -CRI-> cri-o --> runc

在安装docker-ce时，安装的依赖`container.io`就包含它，所以不需要再进行安装，直接配置：

```shell
#生成containerd的配置文件
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml
```

根据文档[Container runtimes](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) 中的内容，对于使用systemd作为init system的Linux的发行版，使用systemd作为容器的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定，因此这里配置各个节点上containerd的cgroup driver为systemd。

修改前面生成的配置文件`/etc/containerd/config.toml`，配置sandbox_image调整pause地址（使用私有仓库地址），并配置SystemdCgroup使用systemd作为容器的cgroup driver

```shell
[plugins."io.containerd.grpc.v1.cri"]
  ...
  sandbox_image = "hub.xxx.com/gcr/google_containers/pause:3.7"
  ...
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      ...
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true
```

然后重启并测试：

```shell
systemctl daemon-reload
systemctl restart containerd
# 测试，确保可以打印出版本信息并且没有错误信息输出
crictl version
#检查containerd日志
systemctl status containerd.service --no-pager -l

#解决crictl命令执行报错`crictl error while dialing dial unix /var/run/dockershim.sock`
crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock
```

crictl可以平替docker（实际是docker-cli）这个指令，大多数参数用法是一样的，参考：[使用 crictl 对 Kubernetes 节点进行调试](https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/crictl/)

## 二、部署kubernetes

安装kubeadm 和 kubelet、kubectl：(使用私有仓库地址)

```shell
echo "deb [trusted=yes] http://apt.xxx.com/repository/aliyun/kubernetes/apt kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
curl -s http://apt.xxx.com/repository/aliyun/kubernetes/apt/doc/apt-key.gpg | apt-key add -
apt update
apt install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl #  标记软件包不被自动更新
```

安装完成后，使用`kubeadm config print init-defaults --component-configs KubeletConfiguration > kubeadm.yml`可以打印集群初始化默认的使用的配置到kubeadm.yml文件，调整该文件如下：

```shell
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  # 修改成master的IP
  advertiseAddress: 192.168.56.109
  bindPort: 6443
nodeRegistration:
  # 设置了容器运行时为containerd
  criSocket: unix:///run/containerd/containerd.sock
  taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.24.3
# 定制了imageRepository为私有仓库的registry
imageRepository: hub.xxx.com/gcr/google_containers
networking:
  podSubnet: 10.244.0.0/16
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# 设置kubelet的cgroupDriver为systemd
cgroupDriver: systemd
failSwapOn: false
```

在开始初始化集群之前可以使用预先拉取所k8s需要的容器镜像：

```shell
#拉取镜像
kubeadm config images pull --config kubeadm.yml
#检查拉取的镜像
crictl images
```

最后，由于我们虚机的多张网卡，k8s节点间通信使用的host-only网卡，这里需要显式让kubelet使用host-only网卡IP：

```shell
echo 'KUBELET_EXTRA_ARGS="--node-ip=192.168.56.109"' > /etc/default/kubelet
#如果直接修改/etc/systemd/system/下的配置，需要重载入配置
#systemctl daemon-reload
#重启服务
systemctl restart kubelet
#检查kubelet进程是否加入--node-ip参数
ps aux | grep kubelet | grep --color node-ip
```

## 三、克隆工作节点

到这一步，需要对所有节点进行安装或配置的工作就完成了，接下来就可以复制master节点生成node1、node2节点。

首相，我们写一个脚本，方便快速对node1、node2的必要配置做更改：

```shell
#!/bin/bash
#filename: init_node.sh

if [ "$(whoami)" = "root" ]; then
    echo "need to run as root"
    exit 1
fi
if [ $# -lt 1 ]; then
    echo "usage: init_node.sh <node_name>"
    exit 1
fi

NODENAME=$1
#调整hostname
hostnamectl set-hostname $NODENAME
sed -i "/^127/ s|master|${NODENAME}" /etc/hosts
#调整kubelet通信ip
nodeIp=$(ip a | grep inet | grep -v 127.0.0.1 | grep -v inet6 | awk '{print $2}' | cut -d / -f1 | grep -e '^192.168')
sed -i "s/192.168.56.109/${nodeIp}" /etc/default/kubelet
```

然后，我们克隆出node1、node2：

![](C:\Users\admin\fun\assets\2022-08-17-09-59-06-image.png)

![](C:\Users\admin\fun\assets\2022-08-17-10-00-34-image.png)

![](C:\Users\admin\fun\assets\2022-08-17-10-01-20-image.png)

## 四、配置集群

master节点上执行：

```shell
# 初始化第一个Master节点. 初始化完成后, 会获得节点ip:port, token和cert-hash用于添加其他master或worker
kubeadm init --config kubeadm.yml
```

等待其初始化完成，如果命令行打印报错，检查kubelet.service运行日志：

```shell
journalctl -xeu kubelet --no-pager
```

排查问题后，重置kubeadm并重新进行初始化：

```shell
kubeadm reset
#重试
kubeadm init --config kubeadm.yml
```

默认init了之后，会提示将kubectl的配置文件拷贝到用户目录，以及打印出来将工作节点加入集群的指令

```shell
# 拷贝kubectl配置文件到用户目录, kubectl默认会使用这个目录下的授权信息访问 Kubernetes 集群
mkdir -p $HOME/.kube/
cp /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```

后续也可以打印工作节点添加命令：token有效期默认是24小时，所以每次使用create新建一个token来打印才是确保有效的。

```shell
kubeadm token create --print-join-command
```

在node1，node2上分别运行以上命令输出的join命令，即可加入k8s集群。然后在master上查看相关信息：

```shell
kubectl get nodes -o wide --all-namespaces
kubectl get pod -o wide --all-namespaces
```

要让Kubernetes Cluster能够工作, 必须安装Pod网络插件, 否则Pod之间无法通信，各node一直处于“Not Ready”状态。这里选择Calico作为网络插件。
```shell
# 下载calico.yaml
curl https://docs.projectcalico.org/v3.9/manifests/calico.yaml -O
# 按需调整镜像源地址，然后执行：
kubectl apply -f calico.yaml
# 网络插件 Calico 安装完之后稍等一会，再次查看 nodes 状态，应该已经变成 Ready
```

默认配置下Kubernetes不会将Pod调度到Master节点, 使用的是Taint标记. 如果希望Master节点也能作为Node使用, 需要先去除该标记。相关指令如下：
```shell
# 使用如下命令去除Taint标记, 加'-'就意味着移除该键
kubectl taint nodes k8s-master node-role.kubernetes.io/master-
# 要恢复Master Only状态, 则使用'=""'添加该键:
kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule
```

如果要更新节点yaml配置，有以下方式：

```shell
# 如果修改的资源有配置灰度更新，直接apply之后由kubenetes进行自动更新
kubectl apply -f xxx.yaml

# 需要手动更新pod的，可以删除pod，由k8s重启启动一个
kubectl delete -n <namespace> pod <pod_name>

# 可以强制刷新配置
kubectl replace --force -f xxx.yaml
```

其他可用的维护指令：

```shell
# 将现有资源导出为yaml, 如deployment（缩写为：deploy）
kubectl get deploy <dp-name> -o yaml

# 查看service（缩写为：svc）限制
kubectl get svc -A
```

## 五、网络插件安装


## 五、dashboard安装

下载dashboard配置文件: https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml

修改该文件: 其

```shell
...
---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort	#新增: 以端口映射方式暴露服务
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30009	#新增: 暴露30009端口
  selector:
    k8s-app: kubernetes-dashboard

---
...
          image: hub.xxx.com/dockerhub/kubernetesui/dashboard:v2.5.0 # 修改docker pull镜像位置
...
          image: hub.xxx.com/dockerhub/kubernetesui/metrics-scraper:v1.0.7 # 修改docker pull镜像位置
...
```

然后执行安装:

```shell
kubectl apply -f ./recommended.yaml
```

然后编写`dashboard-adminuser.yaml`, 创建sa和ClusterRoleBinding, 用于后续生成token登录. (注: `cluster-admin`这个ClusterRole已经在recommended.yaml中创建好了)

```shell
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

然后执行sa创建, 并生成token

```shell
kubectl apply -f dashboard-adminuser.yaml
kubectl -n kubernetes-dashboard create token admin-user
#注: v1.24.0之后, 默认不会再自动生成secret, 也不会绑定到sa; 所以每次要登录, 只能使用create token生成新的token, 这样也更安全
```



## 六、常见错误：

1. Cannot detect current cgroup on cgroup v2
   containerd未运行正常，查看其日志，发现是pause镜像拉取失败，镜像源无法访问

2. `"Error getting node" err="node \"master\" not found"`
   
   containerd未运行正常，导致etcd这个pod没有启动，k8s就无法正常监听配置

3. `Container runtime network not ready`
   未配置网络插件导致，下一节会进行配置

4. 安装dashboard启动报错：CrashLoopBackOff
   
   ```shell
   #查看报错的pod
   kubectl get pods -n kube-system | grep -v Running
   #查看pod存在的问题，发现镜像拉取等没问题，是pod自身运行后异常
   kubectl describe pod <pod-name> -n kube-system
   #查看异常pod的日志，即可定位异常
   kubectl logs <pod-name> -n kube-system
   ```

5. dashboard日志报错：`Metric client health check failed: the server is currently unable to handle the request`
   原因是：指定了使用nodePort，但是没有让dashboard-metrics-scraper使用hostNetwork
   解决方法：

   * 配置dashboard-metrics-scraper，添加`hostNetwork: true`

     ```yaml
     kind: Deployment
     apiVersion: apps/v1
     metadata:
       labels:
         k8s-app: dashboard-metrics-scraper
       name: dashboard-metrics-scraper
       namespace: kubernetes-dashboard
     spec:
       ...
       template:
       ...
         spec:
         ...
           hostNetwork: true
     ```

   * 加载配置，并更新：由k8s内部策略自动更新

     ```shell
     kubectl apply -f ./recommended.yaml
     ```

     

6. dashboard打开后无数据：

   * 需要切换到`kube-system`这个命名空间

   * 可以看下右上角错误提示定位具体错误问题：
     
     * （1）没有ClusterRole：编辑dash_account.yaml加入对具体ClusterRole的创建
     
     * （2）提示没有pods的read权限：修改ClusterRole创建时的resources和verbs配置：
       
       ```shell
       apiVersion: rbac.authorization.k8s.io/v1
       kind: ClusterRole
       metadata:
         # "namespace" 被忽略，因为 ClusterRoles 不受名字空间限制
         name: secret-reader
       rules:
       #使用"*"代表所有资源组：
       - apiGroups: ["*"]
         # 使用"*"代表所有资源
         resources: ["*"]
         verbs: ["get", "watch", "list","create"]
       ```

7. 错误`error execution phase preflight: couldn’t validate the identity of the API Server`: 这是因为使用旧的已过期的token和hash去添加节点. 需要重新生成新的token和hash

8. 默认配置下Kubernetes不会将Pod调度到Master节点, 使用的是Taint标记. 如果希望Master节点也能作为Node使用,

   ```shell
   # 则使用如下命令去除Taint标记, 加'-'就意味着移除该键
   kubectl taint nodes k8s-master node-role.kubernetes.io/master-
   # 要恢复Master Only状态, 则使用'=""'添加该键:
   kubectl taint nodes k8s-master node-role.kubernetes.io/master=NoSchedule
   ```
   
9. 解决重启时containerd-shim服务卡住重启90s问题：
   `vi /etc/systemd/system.conf`调小服务终止等待超时

   ```shell
   DefaultTimeoutStopSec=5s
   ```

10. 私有仓库拉取报错“access denied... authorization failed”
    通过配置secret，指定私仓的用户名密码，注意放到对应的namespace下面：

    ```shell
    kubectl create secret docker-registry <my-secret> -n <my-ns> --docker-server=<my-hub-hostname> --docker-username=<username> --docker-password=<password>
    ```

    然后在需要拉取私仓的资源配置中加入：

    ```yaml
    spec:
      ...
      imagePullSecrets:
      - name: <my-secret>
    ```

    
## 七、组件原理与实现细节

### 全流程体验认识

![](C:\Users\admin\fun\assets\ea2c62caf0c4172943a9833c45c661ebf1b52aee.png)

下面介绍一个Deployment的启动过程:

1. kubectl向APIServer发起创建deployment的请求
2. controller-manager
3. scheduler定时向apiserver获取自己感兴趣的数据, 其中包括待调度的Pod信息和可用的node, 然后进行计算得到调度信息通过apiserver写入etcd
4. Worker节点通过etcd发现有自己有需要部署的Pod, 于是进行Pod的启动操作, 启动完毕则更新etcd中该Pod的信息.

scheduler是k8s的调度模块，做的事情就是拿到pod之后在node中寻找合适的进行适配这么一个单纯的功能。scheduler作为一个客户端，从apiserver中读取到需要分配的pod，和拥有的node，然后进行过滤和算分，最后把这个匹配信息通过apiserver写入到etcd里面，供下一步的kubelet去拉起pod使用。

以v1.2.0版为主, 源码位置在:

* APIServer:
  * kubernetes/cmd/kube-apiserver/app/server.go
  * kubernetes/pkg/apiserver
* kube-scheduler
  * kubernetes/plugin/cmd/kube-scheduler/app/server.go
  * kubernetes/plugin/pkg/scheduler
* kube-controller-manager
  * kubernetes/cmd/kube-controller-manager/app/controllermanager.go
  * kubernetes/pkg/controller

### k8s集群架构

#### APIServer

APIServer是Kubernetes系统管理指令的统一入口. 对任何资源的增删改查操作都需要交给APIServer处理后才能提交etcd.

通过etcdctl连接etcd后台存储, 查看资源信息

```shell
# 首先要安装etcd-client
sudo apt-get install etcd

# 查看, APIServer的资源路径都从/registry开始
etcdctl ls /registry -recursive
```

#### scheduler
scheduler是k8s的调度模块，做的事情就是拿到pod之后在node中寻找合适的进行适配这么一个单纯的功能。scheduler作为一个客户端，从apiserver中读取到需要分配的pod，和拥有的node，然后进行过滤和算分，最后把这个匹配信息通过apiserver写入到etcd里面，供下一步的kubelet去拉起pod使用。

#### kube-controller-manager

kube-controller-manager是Kubernetes集群的控制中心, 负责维护集群的状态, 包括:

* 节点控制器(Node Controller): 负责节点的健康检查和自动恢复
* 副本控制器(Replication Controller): 负责维护Pod的副本数量
* 端点控制器(Endpoint Controller): 负责维护Service的Endpoint
* 服务账号控制器(ServiceAccount Controller): 负责自动创建和更新ServiceAccount
* 密钥控制器(Secret Controller): 负责自动创建和更新Secret
* 配置控制器(ConfigMap Controller): 负责自动创建和更新ConfigMap

#### kubelet

kubelet是Kubernetes集群中每个节点上的代理, 负责维护Pod的生命周期, 包括: 启动、停止、删除等操作. kubelet会定期向APIServer发送节点信息和Pod状态, 并接收APIServer的指令进行操作.
kubelet是唯一没有以容器形式运行的Kubernetes组件, 它是通过Systemd服务运行。


#### kube-proxy
kube-proxy是Kubernetes集群中每个节点上的网络代理, 负责维护Pod的网络规则和转发流量. kube-proxy会定期从APIServer获取Service和Endpoint的信息, 并根据这些信息更新本地的iptables或ipvs规则, 以实现Service的负载均衡和流量转发.

#### etcd
etcd是Kubernetes集群的分布式键值存储系统, 负责存储集群的配置信息和状态信息. etcd会定期向APIServer发送心跳信息, 以确保集群的稳定性和可用性. 

### 服务暴漏
Kubernetes提供了多种方式来暴漏服务, 包括:

* ClusterIP: 在集群内部暴漏服务, 只能在集群内部访问
* NodePort: 在每个节点上暴漏服务, 通过节点的IP和指定的端口访问
* LoadBalancer: 在云环境中使用负载均衡器暴漏服务, 通过负载均衡器的IP访问
* Ingress: 使用Ingress Controller暴漏服务, 通过域名访问

### 网络模型
Kubernetes的网络模型是基于容器网络接口(Container Network Interface, CNI)的, 支持多种网络插件, 包括:

* Calico: 一个开源的网络插件, 支持网络策略和IPAM
* Flannel: 一个简单的网络插件, 支持VXLAN和host-gw两种模式
* Weave: 一个开源的网络插件, 支持VXLAN和UDP两种模式
* Cilium: 一个开源的网络插件, 支持网络策略和IPAM, 支持eBPF和XDP技术

### 存储模型
Kubernetes提供了多种存储方式, 包括:

* 本地存储: 使用节点的本地存储, 例如磁盘或SSD
* 网络存储: 使用网络存储, 例如NFS、Ceph、GlusterFS等

### 日志收集
Kubernetes提供了多种日志收集方式, 包括:

* Fluentd: 一个开源的日志收集器, 支持多种日志格式和输出方式
* Fluent Bit: 一个轻量级的日志收集器, 支持多种日志格式和输出方式
* Logstash: 一个开源的日志收集器, 支持多种日志格式和输出方式
* Elasticsearch: 一个开源的搜索引擎, 支持日志的存储和查询

### 监控分析与告警
Kubernetes提供了多种监控和告警方式, 包括:
* Prometheus: 一个开源的监控和告警系统, 支持多种数据源和告警规则
* Grafana: 一个开源的监控和告警系统, 支持多种数据源和告警规则
* Alertmanager: 一个开源的告警系统, 支持多种告警规则和通知方式

### 安全
Kubernetes提供了多种安全机制, 包括:

* RBAC: 基于角色的访问控制, 支持对资源的细粒度访问控制
* NetworkPolicy: 网络策略, 支持对Pod之间的网络访问控制
* PodSecurityPolicy: Pod安全策略, 支持对Pod的安全配置
* TLS: 传输层安全协议, 支持对数据的加密传输

### 高可用
Kubernetes提供了多种高可用方式, 包括:

* 多Master节点: 使用多个Master节点, 支持故障转移和负载均衡
* 多Region: 使用多个Region, 支持跨地域的高可用
* 多AZ: 使用多个AZ, 支持跨AZ的高可用

### 扩展性
Kubernetes提供了多种扩展方式, 包括:

* CustomResourceDefinition: 自定义资源定义, 支持自定义资源类型
* Operator: 操作符, 支持对自定义资源类型的操作和管理
* Addons: 插件, 支持对集群功能的扩展和增强


## 八、其他资料

k8s概念了解：[概念 | Kubernetes](https://kubernetes.io/zh-cn/docs/concepts/)

k8s官方教程：[教程 | Kubernetes](https://kubernetes.io/zh-cn/docs/tutorials/)

kubectl指令参考：[kubectl 备忘单 | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/)





## 九、维护经验

### kubectl常用控制指令

```shell
# 查看所有pod的详细信息(状态, IP, 所在节点...)
kubectl get -A pod -o wide

# 部署/更新应用, 推荐尽量只使用部署文件+apply

# master上进入某个pod
kubectl exec -it <pod_name> -n <namespace> -- bash

# 查看内部pod的配置
kubectl edit -n kube-system pod kube-apiserver-k8s-master
```

### k8s集群与系统运行协同
1. kube-proxy组件在没有配置conntrack-max时, 默认会根据系统内存设置conntrack-max, 这个功能会导致系统的设置不知不觉被改掉. 详见源码 Kubernetes](https://kubernetes.io/zh-cn/docs/tutorials/)

kubectl指令参考：[kubectl 备忘单 | Kubernetes](https://kubernetes.io/zh-cn/docs/reference/kubectl/cheatsheet/)

