---
layout: post
title: sdk使用cpp重构思路
category: app
typora-root-url: ../../..
---
## 网络事件库

### 事件库选择
【需求】
1. 支持kcp连接, 降低自行合并成本。
2. 较成熟，经过一定规模的线上产品验证。
3. 具有高度可定义特性。

【备选】
* yasio（https://github.com/yasio/yasio）：【√】。
* libhv（https://github.com/ithewei/libhv）：【×】。
* libkcp（https://github.com/xtaci/libkcp）：【√】。

我们最终选择了yasio+libkcp来实现网络事件处理以及kcp+fec封装。不选择libhv原因是其封装相对较复杂，且杂合了c、c++两种代码，显得比较混乱。相较而言，yasio封装相对清 晰，且提供了对于c++14、c++17等新特性的支持。
另外，yasio有更成熟的商业用例，其中集成了Unity、UnrealEngine、axmol等多种引擎的 支持，也可以作为参考。

### 理解yasio事件处理

yasio封装了各平台的同步非阻塞IO复用模型，向上提供Proactor网络模式，仍是以调用回 调函数向上通知网络IO操作结果。

首先要明确yasio抽象出的3个概念含义：
* service: 可以理解为一个事件循环服务，符合`one loop per thread`模式，统一处理该循环服务中的所有连接的网络IO事件、定时器任务等。
* channel: 负责**一个**连接的创建、销毁等连接管理操作。
* transport: 负责**一个**连接的实际数据包收发工作，可以理解为封装连接打开后的句 柄。

下面我们代入到连接全过程看具体的实现细节

1. 事件循环器创建和启动：
```c++
// 创建：会至少关联一个channel
io_service io_();

// 启动：默认创建一个新的线程，执行 io_service::run() 进入事件循环处理，并在事件处理完毕后，调用回调函数进行事件通知。
io_.start([](event_ptr ev) {})
```
这里启动事件循环器后，发现并没有需要处理的事件，线程就会在阻塞在 io_watcher::poll_io() 之上。等待 io_service::wakeup() 唤醒。注意这里 io_watcher 可以根据不同系 统转为不同的IO多路复用实现。

2. 创建一个连接：
使用 io_service::open(channel_idx, kind) 打开通道channel，此时会基于之前设置的地址信息，创建客户端或服务端连接。以下是调用过程：
```shell
io_service::open()
  io_service::open_internal()
    io_service::channel_ops_.push_back(ctx)
    io_service::wakeup()
```
注意到这里只是把channel加入channel_ops_中，并唤醒事件循环器。然后在事件循环器中 执行具体的连接操作：
```shell
io_service::run()
  process_channels()
    # 客户端连接
    do_resolve()
    do_connect()
    do_connect_completion()
    # 服务端连接
    do_accept()
    do_accept_completion()
      handle_connect_succeed()
        active_transport()
          fire_event(..., YEK_ON_OPEN, ...)
```
具体连接创建操作，就是调用底层xxsocket封装，然后填充到transport->socket_中。最后将新的trasnport加入到transports_中，等待事件循环轮询处理。
连接事件创建成功后，会调用`io_service::fire_event()`将相关transport等信息封装进 事件消息中，并在`io_service::dispatch()`中触发上层回调函数进行处理。

3. 连接发送数据包：
```c++
io_service::write(transport, ...)
  transport->write(...)
    send_queue_.emplace(...)
    get_service().wakeup()
```
本质上发送数据包需要调用到transport的发包函数，最终是将数据包加入transport的send_queue_队列中，并触发事件循环，从队列中依次获取数据包进行发送。值得一提的是，这 里的队列采用的是`privacy::concurrent_queue`实现的无锁队列。队列读取数据包发送逻 辑如下：
```c++
io_service::run()
  process_transports()
    do_write(transport)
      transport->do_write()
        io_transport_kcp::do_write()
          ...
            io_transport_kcp::write_cb_
              ::ikcp_send()
              ::ikcp_flush()
                ikcp_output()
                  kcp->output()
                    io_transport_kcp::underlaying_write_cb_
```
这里调用到具体协议类型的transport的do_write()操作后，就不再具体展开了，最终是调 用到具体传输层协议套接字文件描述符的write()。注意到这里kcp协议层通过调整回调函数，在发包过程中巧妙插入了KCP协议处理。


4. 连接接收数据包：
channel创建连接成功后，会调用 io_watcher::mod_event() 对连接对应的socket套接字设置监听可读事件。
当连接可读，就会唤醒事件循环器poll操作，进行数据包接受操作，调用逻辑如下：
```c++
io_service::run()
  process_transports()
    do_read(transport)
      transport->do_read()
        io_transport_kcp::do_read()
          io_transport::call_read()
            read_cb_()
          handle_input()
            ::ikcp_input()
        io_service::unpack()
          fire_event()
```
读取数据包操作相对比较直接，最终在io_service::unpack()中进行粘包处理，然后调用`io_service::fire_event()`将收到的数据包信息封装进事件消息中，触发上层回调处理。

## 改进思路

【集成FEC】

`kcp-go`的作者已经提供了较好的集成FEC的c++版本：[libkcp](https://github.com/xtaci/libkcp.git )。因此，这里可以参考libkcp的已有代码做改进来实现yasio集成FEC：

1. libkcp/sess.cpp：`UDPSession::update` => yasio/io_service.cpp：`io_transport_kcp::handle_input`
2. libkcp/sess.cpp：`UDPSession::out_wrapper` => yasio/io_service.cpp：`io_transport_kcp::io_transport_kcp()`中的`::ikcp_setoutput()`所设置的发包回调函数。

注意：

* 需使用最新版本的kcp库，较旧版本可能有内存泄漏问题，大压力请求下会导致SIGKILL问题。



【支持自定义测速与建连】

可以在两种场景进行自定义测速：

1. 探测就近接入点：这种场景接入点收到UDP协议包后直接回包，不转发到后端处理。考虑：

   * 如何高效回包：可以使用ebpf直接在网卡层快速回包，处理逻辑简单。
   * 是否有必要避免被利用实施攻击：简单的回显服务收发流量为1:1流量，并不会放大攻击，等量攻击较难实施，基本不用担心被利用的问题。

2. 探测最快连接：这种场景接入点收到UDP协议包需要转发到后端目标服进行回显。考虑：

   * 如何实现**动态转发**：可以在探测协议包中加入后端目标服的IP:Port信息，这样探测协议包到达接入点，接入点就可以解析得到后端目标服的IP:Port信息，构造并保存转发规则，即可实现动态转发。

   * 如何实现**透传真实地址**信息：可以在探测协议包中提前保留真实地址信息IP:Port字段，当数据包到达接入点后，接入点获取连接的对端地址即为真实地址信息，填入预留的IP:Port字段即可。

   * 探测最快的连接可以被复用：在探测协议包中传入客户端生成的conv，作为该连接的会话，即可实现连接复用。

   * 如何高效转发包：可以使用ebpf开发转发程序，注意：

     对于UDP协议，当前可以完全在内核态基于XDP实现动态转发包。有几个技术性突破问题解决思路如下：

     * (1) 转发包后对于后端目标服地址如何找到路由：使用`bpf_fib_lookup()`查找路由，参考[xdp4slb](https://github.com/MageekChiu/*xdp4slb*)、[udplb](https://github.com/moolen/udplb)
     * (2) 如何找到可用的源端口：
       * 初始时，可以限定内核可用端口，划分出一大段端口如`30000~60000`给XDP程序使用。
       * XDP程序内部循环次数受限，因此我们可以采用`BPF_MAP_TYPE_QUEUE`类型的map数据结构来记录可用端口。初始时，分配器 i = 30000，当需要分配端口时就累计分配器。当有连接断开时，回收源端口并放入可用端口QUEUE。
       * 当 `i == 60000`时，说明所有端口已经记录进系统，此时如果还需分配端口则从可用端口QUEUE中出队，如果出队出错，说明已无可用端口。

     对于TCP协议，由于我们需要等待客户端与接入点三次握手后，接入点收到第一个探测协议包，才能知道如何转包。在内核态维护TCP连接状态比较复杂，所以这里我们采用如下方式：

     * (1) 首先，在接入点用户态接受TCP连接（记为conn1），解析探测协议包，并动态创建接入点与后端目标服的TCP连接（记为conn2）。
     * (2) 将上述两个连接的对应关系记录进内核`BPF_MAP_TYPE_SOCKHASH`类型的map数据结构。并编写`sk_skb/stream_verdict`类型ebpf钩子程序，实现两个连接的socket队列数据转发offload在内核态。参考：[[译] 利用 ebpf sockmap/redirection 提升 socket 性能（2020）](http://arthurchiao.art/blog/socket-acceleration-with-ebpf-zh)
     * (3) 用户态补充发送探测协议包到conn2。
     * (4) 此后，conn1、conn2收到数据包就会自动触发ebpf钩子，使用`bpf_sk_redirect_hash`查询SOCKHASH，如果找到对端socket则自动将数据包放入该socket发包队列，实现数据包自动转发。



【实现FEC码率自动调整】

1. 实现decoder自动对齐encoder码率：可以参考kcp-go的autotune实现。当出现码率对齐变动时，本质上是在收包过程中解码前的节点，重新按新的码率构建一个decoder替换旧的。

2. 实现encoder码率变动：在发包过程中编码前的节点，重新按新的码率构建一个encoder替换旧的。由于kcp协议收发包都是单线程处理，因此不会有问题，否则需要考虑互斥替换问题。

3. 如何判断码率是否需要变动：首先需参考kcp-go补充必要的snmp数据统计，然后就是码率计算算法。这里FEC码率是为了对抗丢包的，因此我们可以直接将FEC码率与当地线路丢包率对齐匹配。

   首先是基于snmp数据计算丢包率，这里忽略多个seg被合并到一个pkt的情况，则可以近似这么计算：

   * 丢包数 = 重传segs数(RetransSegs - old_RetransSegs) + FEC恢复包数(FECRecovered - old_FECRecovered)
   * 发包数 = 成功发包数(OutPkts - old_OutPkts)
   * 丢包率 = 丢包数 / 发包数

   注意我们统计的是一段时间的丢包率，所以上述使用的是对应的指标增量值。

   然后，我们可以将丢包率分级别，匹配不同的码率进行抗丢包，如下：

   * 丢包率0%~4%，选用data_shard=24，parity_shard=1，可以对抗4%的丢包。
   * 丢包率4%~10%，选用data_shard=9，parity_shard=1，可以对抗10%的丢包。
   * 丢包率10%~20%，选用data_shard=4，parity_shard=1，可以对抗20%的丢包。
   * 丢包率20%~30%，选用data_shard=7，parity_shard=3，可以对抗30%的丢包。
   * 丢包率30%~40%，选用data_shard=3，parity_shard=2，可以对抗40%的丢包。
   * 丢包率40%~50%，选用data_shard=1，parity_shard=1，可以对抗50%的丢包。
   * 丢包率50%~60%，选用data_shard=2，parity_shard=3，可以对抗60%的丢包。
   * 丢包率>60%，选用data_shard=1，parity_shard=3，可以对抗最大75%的丢包。

   最后，本端确定好码率后，可以将码率以一个8bit字段的方式更新到FEC包头中，实时同步到对端，确保两端encoder码率一致。

   另外，为了防止码率变动过于频繁，可以参考采用如下策略：

   * 如果计算到丢包率在上升，立即提升码率。
   * 如果计算到丢包率下降，则需综合前几次丢包率数据（可以保存到定长队列里）判断是否需要降低码率，防止反弹。注意，如果丢包率贴近边界值也不做调整，可以设定只有低于边界值5%以上再调整。
   * 码率调整后的效果需要根据实际运行情况来逐步确定，因此需要搭建模拟环境，并配合丢包模拟工具模拟不同丢包率情况，反复验证。