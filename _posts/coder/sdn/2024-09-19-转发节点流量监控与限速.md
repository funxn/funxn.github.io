---
layout: post
title: 转发节点流量监控与限速
categories: [coder, sdn]
tags: [sdn, 流量监控, 流量限速]
date: 2024-09-19 16:00:00 +0800
---

## 背景

各机房出口带宽是有限的，为防止低优先级业务抢占机房出口带宽，进而影响高优先级业务数据传输导致丢包，需要实现流量监控和流量限速：
1. 流量监控：监控目的是发现不合理占用带宽的大象流。
2. 流量限速：限速是通过一定策略对连接中的数据包进行适当延迟或丢包，以降低该连接对带宽的占用。

## 流量监控
实现流量监控，一般架构为：`sflow-agent + sflow-collector + 后端分析展示服`。以下是一种可行方案是：
* sflow-agent选择：hsflowd；
* sflow-collector选择：elasticflow；
* 后端分析展示服选择：elasticsearch存储采样数据，kibana做统计分析与展示。

elasticflow也可以接收netflow采样，不过netflow在大流量情况下有一定性能问题，且无法实时进行展示，因此我们还是选择hsflowd。

详细的部署过程可以参考：[CentOS7 部署ElastiFlow网络流量分析平台](https://cloud.tencent.com/developer/article/2004769)

### hsflowd流量采样注意
hsflowd实际部署过程中，还是比较容易遇到问题的，这里统一记录一下。

【如何排查日志】
hsflowd默认会将日志输出到系统日志中，不过如果没有开调试日志，基本查不到问题。打开调试日志可以是用这样的方式：
```shell
hsflowd -ddd 1>/path/to/logfile 2>&1 &
```

【启动后上行带宽大量占用】
按照网上常用的配置，在kvm虚拟机运行后，发现上行带宽立即占用了100Mbps左右。经排查是sflow协议包过多导致。需要调整采样频率，加大sampling数值，适当减小headerBytes数值。
```
sflow {
  DNSSD = off
  polling = 10
  sampling = 64
  collector {
    ip=XX.XX.XX.XX
    udpport=6343
  }
  headerBytes = 128
}
```

【vmware虚拟机采样包较少】
全服安装hsflowd后，发现相同流量的kvm虚拟机和vmware虚拟机，vmware虚拟机采集到的包数量很少。后分别开启调试日志进行对比，发现kvm虚拟机因为网卡速率匹配失败用了默认的`sampling`采样率，而vmware虚拟机由于网卡速率匹配成功，匹配到的实际采样率是`sampling.10G`=10000，从而导致采样包过小。所以除了配置`sampling`，还需配置`sampling.<speed>`。如下：
```
sflow {
  ...
  sampling = 64
  sampling.100M = 64
  sampling.1G = 64
  sampling.10G = 64
  sampling.40G = 64
  ...
}
```


## 连接限速
连接限速的有种实现方式，对比如下：
* 使用ovs的meter表：存在一定的性能瓶颈，且最终还是调用tc进行限速。
* 直接使用tc：发展时间长，性能好，稳定性好。

我们选择使用tc进行限速。关于tc指令限速的基本知识就不详细介绍，主要是掌握：
* qdisc、class、filter的概念。参考：[[译] 《Linux 高级路由与流量控制手册（2012）》第九章](https://arthurchiao.art/blog/lartc-qdisc-zh/)
* 有类别qdisc、无类别qdisc：有类别qdisc可以挂接class，class可以挂接qdisc；无类别qdisc不能挂接class。最新的qdisc推荐是直接查看man手册：`man 8 tc`。
* 各qdisc的操作，参数。直接查看man手册，如：`man 8 tc-hfsc`
* 子类别、叶类别区分：部分qdisc队列规则，如：htb，有子类别和叶类别之分，子类别是父类别下的子类别，子类别可以挂接qdisc，而叶类别不能挂接qdisc，只能挂接filter。

这里着重记录下实践过程中遇到的问题。

### 如何实现分级限流
#### 采用htb
使用htb排队规则，再按需将总带宽分到三个优先级分类，每个优先级分类挂接不同的排队规则。如下：
```shell
# 根队列采用htb排队规则，默认数据包发送到1:10类别。
tc qdisc add dev eth0 root handle 1: htb default 10
# 在根1:0下挂接1:1类别，限制总流量上限
tc class add dev eth0 parent 1: classid 1:1 htb rate 1000kbps ceil 1500kbps
# 在1:1类别下挂接1:10子类别，限制使用速率1kbps，上限2kbps。由于1:10类别下没有挂接qdisc，默认使用qfifo排队规则。
tc class add dev eth0 parent 1:1 classid 1:10 htb rate 1kbps ceil 2kbps
# 在1:1类别下挂接1:11子类别，限制使用速率1kbps，上限200kbps；同时挂接fq排队规则，同时限制每条连接最大速率为10kbps，标记为11:。
tc class add dev eth0 parent 1:1 classid 1:11 htb rate 100kbps ceil 200kbps
tc qdisc add dev eth0 parent 1:11 handle 11: fq maxrate 10kbit

# 将匹配源ip的流量导入类1:11中进行限流，实现每条连接公平限速10kbps
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src ${CLIENT_IP} flowid 1:11
```
注意：
* 使用htb分级限流有一个弊端，当**队列总流量超过200MBps，htb在各队列间流量分配会不准确**，需要考虑采用其他分级限流排队规则，推荐hfsc。
* sfq排队规则可以保证各连接公平分享带宽，但**无法限制单条连接的最大速率**，因此推荐采用fq排队规则。

#### 采用hfsc
使用hfsc排队规则，再按需将总带宽分到三个优先级分类，每个优先级分类挂接不同的排队规则。如下：
```shell
# 根队列采用hfsc排队规则，默认数据包发送到1:10类别。
tc qdisc add dev eth0 root handle 1: hfsc default 10
# 在根1:0下挂接1:1类别，限制总流量上限
tc class add dev eth0 parent 1: classid 1:1 hfsc sc m2 1500mbit
# 在1:1类别下挂接1:10子类别，限制速率2mbps，上限2mbps（无法借用父类别流量）。由于1:10类别下没有挂接qdisc，默认使用qfifo排队规则。
tc class add dev eth0 parent 1:1 classid 1:10 hfsc sc m2 2mbit ul m2 2mbit
# 在1:1类别下挂接1:11子类别，限制使用速率200mbps，上限1500mbps（可以从父类别中借用流量）；同时挂接fq排队规则，同时限制每条连接最大速率为10mbps，标记为11:。
tc class add dev eth0 parent 1:1 classid 1:11 hfsc sc m2 200mbit ul m2 1500mbit
tc qdisc add dev eth0 parent 1:11 handle 11: fq maxrate 10mbit
# 将匹配源ip的流量导入类1:11中进行限流，实现每条连接公平限速10mbps
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src ${CLIENT_IP} flowid 1:11
```
注意：
* hfsc的类别限速的配置参考`man 8 tc-hfsc`。注意点：
  * 限速曲线分为即时（real-time，rt）和连接共享（link-sharing, ls）两种标准ls标准下，每个类别可以借用父类别的流量，而rt标准下，每个类别只能使用自己的流量。参考`man 7 tc-hfsc`，**ls曲线只能用于子类别**，所有其他曲线设置都会转为ls曲线；另外区分设置ls曲线或rt曲线容易导致意料之外的结果，因此**推荐直接使用`sc`曲线模式，由hfsc自行调节**。
  * 配置`m1`、`m2`阶段斜率时，但**通常只需配置`m2`参数，`m1`默认等于`m2`，表示使用默认斜率**。m1、m2参数配置为不同值表示使用自定义斜率。`d`参数省略则表示时间区间为1s。
  * `ul m2 <BPS>`表示使用父类别的流量上限，如果不设置，则使用父类别的`m2`参数，默认是不限制。

### tc限速问题排查
tc命令行工具没有提供日志，因此排查问题方式不太直接，以下总结自己使用的两种方式：
#### 方式一：查看tc限速状态
命令行工具提供了`tc -s`参数，可以查看当前tc限速状态。状态输出中的各字段含义按qdisc类型不同而不同，大致可以参考：https://shorewall.org/traffic_shaping.htm

另外，tc命令官方文档见：https://lartc.org/lartc.html

#### 方式二：使用ebpf工具
ebpf可以在tc中的qdisc、class、filter中插入探针，从而获取到qdisc、class、filter的运行状态。参考：https://eunomia.dev/zh/tutorials/20-tc/。