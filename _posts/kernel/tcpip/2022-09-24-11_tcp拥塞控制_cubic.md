---
layout: post
title: 11_tcp拥塞控制
categories: [kernel, tcpip]

---



### 前言

#### 接收窗口与拥塞窗口关系

开始具体介绍这些算法之前，需要先对几个名词进行一下解释

* rwnd: 接收窗口，由 TCP 的接收端确定
* cwnd: 拥塞窗口，由 TCP 的发送端视当前网络状况而定

rwnd 说明了接收端当前可以接收的最大数据量，cwnd 说明了当前网络情况允许发送的最大数据量。因此，**实际可以发送的最大数据量就是 min(rwnd, cwnd)**

* MSS (Maximum Segment Size): 最大分段大小
* RTT (Round Trip Time): 网络往返时间
* RTO (Retransmission TimeOut): 重传超时时间
* ssthresh: 慢启动门限，如果当前拥塞窗口小于此值则触发慢启动（就是快速增长），其初始值是由系统定义的

扩展：滑动窗口机制
TCP 连接的两端通过滑动窗口来动态的调节数据的发送与接收，以保证数据被正确接收而不会因为过多的发送而被淹没；还可以在数据报丢失的时候进行重传，以尽力使数据送达。
滑动窗口机制的核心思想是：发送方在发送数据的同时，为每个字节都维护一个计时器，当计时器超时的时候，如果没有收到该字节的 ACK，则认为该数据包丢失，需要重新发送。接收方在收到数据的时候，会返回一个 ACK 包，其中包含一个窗口大小字段，表示接收方当前可以接收的数据量，发送方收到 ACK 之后，会根据窗口大小字段来调整自己的发送窗口大小，从而动态的调整发送速率，以适应网络状况的变化。

#### 拥塞控制算法

linux通过比较snd.nxt和snd.una的差值来确定当前有多少仍在网络传输中并且未得到确认的段, 然后通过比较拥塞窗口(cwnd)和这些传输中的段的数量, 来确定有多少可以进行发送. 可发送包数量等于拥塞窗口剩余段数量.

而TCP规范和其他一些实现, 比较的是cwnd和传输中的字节数, 每个拥塞窗口段则允许发送多个小包, 相比linux实现在拥塞情况下能发送更多包.

ref: https://www.cnblogs.com/zlingh/p/6161088.html

我接触过的TCP拥塞控制算法有这四种：
* reno: 是TCP拥塞控制的一个经典算法，它包括慢启动、拥塞避免和快速恢复三个阶段。在慢启动阶段，拥塞窗口大小成倍增长，直到达到一个阈值或检测到丢包。当检测到丢包时，Reno算法会将拥塞窗口减半，并进入快速恢复阶段。
* new reno：是Reno算法的一个改进版本，它在快速恢复阶段引入了快速重传机制，即在检测到连续三个重复的ACK时，立即重传丢失的数据包，而不是等待RTO超时。
* bic/cubic：是Reno算法的一个改进版本，旨在优化高速高延迟网络的拥塞控制。它在**拥塞避免阶段**引入了Bic/Cubic算法，BIC使用二分搜索算法来寻找最佳的拥塞窗口大小，而CUBIC则使用三次函数来模拟和预测窗口大小的变化。

* bbr：是Google提出的一种新的拥塞控制算法，它通过动态调整拥塞窗口的大小和发送速率来优化网络传输性能。

我们通常所说的TCP拥塞控制算法，一般是指reno算法，也就是经典的TCP拥塞控制算法。本文则**重点分析当前linux系统默认使用的Cubic算法**。

**如何在用户态获得拥塞窗口**

1. 在内核的函数tcp_getsockopt的代码中，可以看到这个选项TCP_INFO，返回了几乎所有的参数，同时还有其他的许多参数可以得到一些其他的信息。具体每个参数的含义可以参考内核中的注释。
2. 使用 sysctl 命令查看内核参数，例如sysctl net.ipv4.tcp_congestion_control，可以查看当前使用的拥塞控制算法。


### 定义

拥塞控制：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。

流量控制：指点对点通信量的控制. 流量控制所要做的\就是抑制发送端发送数据的速率, 以便使接收端来得及接收

拥塞控制的控制方法:

慢启动(slow-start), 拥塞避免(congestion avoidance), 快重传(fast retransmit)和快恢复(fast recovery)

### 拥塞控制的基本结构

![tcp_reno_congest](/img/tcpip/tcp_reno_congest.png)

拥塞控制的各种变量保存在tcp_sock结构中. 

#### 一. 慢启动

**慢启动门限值ssthresh(即: slow start thresh)**

内核中的ssthresh保存在tcp_sock结构中的snd_ssthresh字段, 其初始值在

```shell
net/ipv4/tcp_ipv4.c::tcp_v4_init_sock()
  tp->snd_ssthresh = TCP_INFINITE_SSTHRESH;
```

CUBIC算法慢启动门限ssthresh在两种情况下会得到更新. (1)是在接收到ack应答包, 对应处理函数为bictcp_acked(); (2)是在发生拥塞时, 慢启动门限回退, 对应处理函数为bictcp_recalc_ssthresh()

```c
static struct tcp_congestion_ops cubictcp __read_mostly = {
	//CUBIC算法变量初始化，在tcp三次连接时，回调用其初始化
    // 套接字的拥塞控制变量
	.init		= bictcp_init,
    //拥塞时慢启动门限回退计算
	.ssthresh	= bictcp_recalc_ssthresh,
    //拥塞避免
	.cong_avoid	= bictcp_cong_avoid,
    //如果拥塞状态是TCP_CA_Loss，Reset拥塞算法CUBIC的各种变量
	.set_state	= bictcp_state,
    //拥塞窗口回退
	.undo_cwnd	= bictcp_undo_cwnd,
    //当tcp_ack调用tcp_clean_rtx_queue将收到应答的数据包从重传
    // 队列删除时，会调用bictcp_acked更新慢启动阈值
	.pkts_acked     = bictcp_acked,
	.owner		= THIS_MODULE,
	.name		= "cubic",
};
```

下面追踪情况(1)更新tcp_sock.snd_ssthresh的过程

```shell
net/ipv4/tcp_input.c::tcp_ack()
  tcp_clean_rtx_queue()
    ca_ops->pkts_acked() => net/ipv4/tcp_cubic.c::bictcp_acked()
      net/ipv4/tcp_cubic.c::hystart_update()
```

初始snd_ssthresh的值是很大的, bictcp_acked()调用hystart_update()更新snd_ssthresh, 需要满足`snd_cwnd >= hystart_low_window`这一条件, 而cubic算法设置其为16. 这样, 慢启动算法的初始门限值其实就是reno算法中常见的=16

注: cubic模块参数hystart_low_window可以通过如下方式更新设置, 其他模块参数设置方式类似:
`echo 128 > /sys/module/tcp_cubic/parameters/hystart_low_window`

**慢启动阶段snd_cwnd更新**

下面探究慢启动情况下snd_cwnd的值如何增长到触发门限制更新的

```shell
net/ipv4/tcp_input.c::tcp_ack()
  tcp_cong_avoid()
    icsk->icsk_ca_ops->cong_avoid() =>
    net/ipv4/tcp_cubic.c::bictcp_cong_avoid()
      net/ipv4/tcp_cong.c::tcp_slow_start()
```

tcp_ack()检测到丢包(即收到连续3个重复ack), 就会进入拥塞处理阶段. 正常情况下, tcp_slow_start()函数中会将snd_cwnd每收到一个ack就加1, 这样经过一轮RTT, 拥塞窗口就会以2次方方式增长.

#### 二. 拥塞避免

上面看到, 慢启动阶段窗口增长是按照标准的tcp拥塞控制方式增长的. 真正体现cubic算法不同之处的阶段, 正是拥塞避免阶段. CUBIC中最关键的点在于它的窗口增长函数仅仅取决于连续的两次拥塞事件的时间间隔值, 从而窗口增长完全独立于网络的时延RTT, 这使得CUBIC能够在多条共享瓶颈链路的TCP连接之间保持良好的RTT公平性?!

$$
K = \sqrt[3]{ \frac{W_{max}\beta} {C} } , \\
W(t) = C(t - K)^3 + W_{max}
$$

* W<sub>max</sub>: 为此时发生拥塞时的窗口值
* β: 乘法减小因子
* C: cubic参数
* t: 下一个rtt所处的时间
* W(t): 即下一个rtt时拥塞窗口的大小
* K: 拥塞窗口从W(t)增长到W<sub>max</sub>所需的时间

拥塞避免阶段的函数调用逻辑

```shell
net/ipv4/tcp_input.c::tcp_ack()
  tcp_cong_avoid()
    icsk->icsk_ca_ops->cong_avoid() =>
    net/ipv4/tcp_cubic.c::bictcp_cong_avoid()
      net/ipv4/tcp_cubic.c::bictcp_update()
      net/ipv4/tcp_cong.c::tcp_cong_avoid_ai()
```

其中bictcp_update()的任务就是计算ca->cnt, 而tcp_cong_avoid_ai()则通过比较tp->snd_cwnd_cnt和ca->cnt比较, 确定是否增加拥塞窗口tp->snd_cwnd, 注意在增加拥塞窗口前, 还会判断当前拥塞窗口是否大于拥塞窗口允许的最大值tp->snd_cwnd_clamp, 其值在tcp_v4_init_sock()中初始化, 默认为~0, 即2^32-1

ca->cnt用于控制拥塞窗口的增长速率, 可以理解为每经过ca->cnt个包拥塞窗口就加1. 而bictcp_update()就是依照上述公式来计算这个值的. 通过读该函数代码, 可以总结出如下特点:

* 初始时, W<sub>max</sub>取值为当前拥塞窗口大小, 所以计算出的ca->cnt很大, 拥塞窗口增长会很慢. 由于设置了tcp_friendliness参数, 所以会使用Reno算法增长拥塞窗口
* 当发生拥塞时, W<sub>max</sub>会取得拥塞发生时的拥塞窗口值, 拥塞窗口则通过快速恢复进行调整(见下文). W<sub>max</sub>和cwnd的差值会很大, 所以计算出来的ca->cnt就会比较小, 于是拥塞窗口的增长就会很快回复到接近拥塞发生时的大小. 此时又会恢复缓慢增长.

#### 三. 快重传

到了一步, 我们就需要关注拥塞处理的当前状态了, 然后依据当前拥塞状态决定下一步的处理动作. 所有的拥塞状态处理都集中在tcp_fastretrans_alert()这个函数中, 该函数在如下几种条件下会被调用:

 * each incoming ACK, if state is not "Open"

 * when arrived ACK is unusual, namely:

   ​	(1)SACK
   ​	(2)Duplicate ACK
   ​	(3)ECN ECE

调用逻辑如下:

```shell
net/ipv4/tcp_input.c::tcp_rcv_established()
  tcp_ack()
    tcp_ack_is_dubious()
    tcp_fastretrans_alert()
```

内核中拥塞处理状态主要有如下几种(参见内核代码注释"Linux NewReno/SACK/FACK/ECN state machine")

> "Open"	Normal state, no dubious events, fast path.
>
> "Disorder"   In all the respects it is "Open", but requires a bit more attention. It is entered when we see some SACKs or dupacks. It is split of "Open" mainly to move some processing from fast path to slow one.
>
> "CWR"	CWND was reduced due to some Congestion Notification event. It can be ECN, ICMP source quench, local device congestion.
>
> "Recovery"	CWND was reduced, we are fast-retransmitting.
>
> "Loss"	CWND was reduced due to RTO timeout or SACK reneging.

在第一个重复的ACK到达后, 发送方即进入Disorder状态. 在有三个连续的重复ACK到达后, 发送方重传地一个没有被确认的段, 然后从Disorder状态进入Recovery状态. 此时ssthresh被重新计算, cubic中为max(0.7*snd_cwnd，2). 这段代码在

```shell
net/ipv4/tcp_input.c::tcp_time_to_recover()
```

在Recovery状态期间, 拥塞窗口cwnd是每隔一个新到的确认就减小一个段, 一直减小到ssthresh. 这是在

```shell
net/ipv4/tcp_input.c::tcp_update_cwnd_in_recovery()
```



#### 四. 快恢复

```shell
net/ipv4/tcp_input.c::tcp_fastretrans_alert()
  # 执行状态撤销
  tcp_try_undo_recovery()
    tcp_undo_cwr()
      icsk->icsk_ca_ops->undo_cwnd =>
      net/ipv4/tcp_cubic.c::bictcp_undo_cwnd()
  tcp_complete_cwr()
  # 执行状态转移
  tcp_add_reno_sack()
  tcp_timer_to_recover()
  icsk->icsk_ca_ops->ssthresh =>
  net/ipv4/tcp_cubic.c::bictcp_recalc_ssthresh()
  # 更新拥塞窗口, 重传被标记为LOST的段
  tcp_update_cwnd_in_recovery()
  net/ipv4/tcp_output.c::tcp_xmit_retransmit_queue()
```

Recovery状态时, 拥塞窗口会一直减小, 直到为ssthresh为止. 然后会等待所有拥塞发生时所有未完成确认的段都被确认了, Recovery就会恢复到Open状态, 之后就走的是快恢复流程了.

如果所有未完成确认的段并没有都被确认, 则Recovery状态会转入Loss状态, ssthresh会被修改为1, 当Loss状态重传完毕恢复到Open状态时, 之后走的就是慢启动流程.

问题: Recovery状态下的快重传在哪里计数和实现? 计数通过tcp_add_reno_sack(), 重传是通过tcp_update_scoreboard()标记LOST然后在tcp_xmit_retransmit_queue()中进行重传.



**总结:**

拥塞控制中的状态主要有5种, 为Open, Disorder, CWR, Recovery, Loss; 其中CWR为路由显式通告拥塞, 这里不关注. 下面总结下4种状态转换的时机: 

(0)初始时, 拥塞状态为Open, 拥塞窗口增长会按照前面

(1)在第一个重复的ACK到达后, 发送方即进入Disorder状态. 在有三个连续的重复ACK到达后, 发送方重传第一个没有被确认的段, 然后从Disorder状态进入Recovery状态. 此时ssthresh被重新计算, cubic中为max(0.7*snd_cwnd，2)

(2)Recovery状态时, 拥塞窗口会一直减小, 直到为ssthresh为止. 然后会等待所有拥塞发生时所有未完成确认的段都被确认了, Recovery就会恢复到Open状态, 之后就走的是快恢复流程了.

(3)如果所有未完成确认的段并没有都被确认, 则Recovery状态会转入Loss状态, ssthresh会被修改为1, 当Loss状态重传完毕恢复到Open状态时, 之后走的就是慢启动流程.



### 拥塞控制处理中的状态迁移

为什么需要状态迁移? 从上面的拥塞控制中, 